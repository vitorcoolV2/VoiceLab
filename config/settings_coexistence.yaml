# Coexistence Configuration - Optimized for sharing GPU with Ollama Phi-3
# This configuration uses minimal GPU memory to allow both services to run

api:
  max_text_length: 1000
  rate_limit: 100
  timeout: 30

audio:
  normalize: true
  quality: low  # Lower quality for faster processing
  sample_rate: 16000  # Lower sample rate
  trim_silence: true

logging:
  backup_count: 5
  file: ./logs/tts.log
  format: '{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}'
  level: INFO
  max_size: 10MB

models:
  auto_download: true
  cache_dir: /media/vitor/ssd990/ai_models/cache
  path: /media/vitor/ssd990/home-backup/coqui-models

output:
  filename_template: speech_{timestamp}_{hash}.wav
  max_file_size: 50MB  # Reduced file size
  path: /media/vitor/ssd990/home-backup/coqui-outputs

downloads:
  path: /media/vitor/ssd990/home-backup/coqui-downloads

# COEXISTENCE MODE - Minimal GPU usage
performance:
  batch_size: 1  # Single batch to minimize memory
  gpu_memory_fraction: 0.3  # Only 30% of GPU memory
  max_queue_size: 5  # Smaller queue
  use_gpu: true

security:
  enable_auth: false
  secret_key: your-secret-key-here
  token_expiry_hours: 24

server:
  debug: false
  host: 0.0.0.0
  port: 8000
  workers: 1  # Single worker to reduce memory

tts:
  default_model: tts_models/multilingual/multi-dataset/xtts_v2
  default_output_format: wav
  default_pitch: 1.0
  default_speaker: null
  default_speed: 1.2  # Slightly faster for efficiency
  default_voice_sample: /secure/flows-feedback-loop/coqui-tts/output6.wav
  default_voice_style: normal
  default_volume: 1.0
  use_default_voice_sample: true

# Fastest Whisper model for coexistence
whisper:
  available_models:
  - tiny
  - base
  - small
  - medium
  - large
  - large-v2
  compression_ratio_threshold: 2.4
  default_model: tiny  # Fastest model
  force_cpu: false
  language_detection: true
  logprob_threshold: -1.0
  no_speech_threshold: 0.6
  temperature: 0.0
  word_timestamps: false 