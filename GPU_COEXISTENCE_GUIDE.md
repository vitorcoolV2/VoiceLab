# üéõÔ∏è GPU Coexistence Guide: Coqui TTS + Ollama Phi-3

Este guia explica como configurar o Coqui TTS para coexistir com Ollama Phi-3 na mesma GPU NVIDIA GeForce RTX 3060.

## üìä Situa√ß√£o Atual

- **GPU**: NVIDIA GeForce RTX 3060 (12GB VRAM)
- **Coqui TTS**: ~2.1GB VRAM (modo coexist√™ncia)
- **Ollama Phi-3**: ~2-4GB VRAM (estimado)
- **Sistema**: ~100MB VRAM
- **Total Dispon√≠vel**: ~5.7GB para outros processos

## üéØ Configura√ß√µes Dispon√≠veis

### 1. üöÄ **Speed Mode** (Dedicated GPU)
**Use quando**: Ollama n√£o est√° a correr ou precisa de m√°xima performance

```bash
python scripts/switch_config.py speed
```

**Configura√ß√µes**:
- GPU Memory: 85% (10.4GB)
- Batch Size: 4
- Audio Quality: Medium
- Workers: 2
- Performance: ~15-18 segundos

### 2. ü§ù **Coexistence Mode** (Shared GPU)
**Use quando**: Ollama Phi-3 est√° a correr

```bash
python scripts/switch_config.py coexistence
```

**Configura√ß√µes**:
- GPU Memory: 30% (3.7GB)
- Batch Size: 1
- Audio Quality: Low
- Workers: 1
- Performance: ~17-20 segundos

## üîÑ Como Alternar Configura√ß√µes

### Detec√ß√£o Autom√°tica
```bash
python scripts/switch_config.py
```
O script detecta automaticamente se o Ollama est√° a correr e recomenda o modo apropriado.

### Altern√¢ncia Manual
```bash
# Para coexist√™ncia com Ollama
python scripts/switch_config.py coexistence

# Para velocidade m√°xima
python scripts/switch_config.py speed
```

## üìà Performance Comparison

| Mode | GPU Memory | Batch Size | Quality | Time | Use Case |
|------|------------|------------|---------|------|----------|
| **Speed** | 85% (10.4GB) | 4 | Medium | ~17s | Dedicated GPU |
| **Coexistence** | 30% (3.7GB) | 1 | Low | ~18s | Shared with Ollama |

## üõ†Ô∏è Otimiza√ß√µes Aplicadas

### Para Coexist√™ncia:
1. **GPU Memory Fraction**: 30% (vs 85% no speed mode)
2. **Batch Size**: 1 (vs 4 no speed mode)
3. **Audio Quality**: Low (vs Medium no speed mode)
4. **Sample Rate**: 16kHz (reduzido de 22kHz)
5. **Workers**: 1 (vs 2 no speed mode)
6. **Whisper Model**: Tiny (mais r√°pido)

### Para Speed:
1. **GPU Memory Fraction**: 85% (m√°ximo uso)
2. **Batch Size**: 4 (processamento paralelo)
3. **Audio Quality**: Medium (melhor qualidade)
4. **Workers**: 2 (m√∫ltiplos workers)
5. **Queue Size**: 30 (maior capacidade)

## üîç Monitoriza√ß√£o

### Verificar Uso da GPU
```bash
nvidia-smi
```

### Verificar Status do Servidor
```bash
curl -s http://localhost:8000/health
```

### Verificar Se Ollama Est√° a Correr
```bash
pgrep ollama
```

## üö® Troubleshooting

### Problema: GPU Out of Memory
**Solu√ß√£o**: Mudar para modo coexist√™ncia
```bash
python scripts/switch_config.py coexistence
```

### Problema: Performance Lenta
**Solu√ß√£o**: Verificar se Ollama est√° a usar muita mem√≥ria
```bash
nvidia-smi
# Se Ollama usar >4GB, considere parar temporariamente
```

### Problema: Servidor N√£o Inicia
**Solu√ß√£o**: Reduzir ainda mais a mem√≥ria GPU
```bash
# Editar config/settings_coexistence.yaml
# Mudar gpu_memory_fraction: 0.2 (20%)
```

## üìã Checklist de Configura√ß√£o

### Para Coexist√™ncia:
- [ ] Ollama Phi-3 instalado e configurado
- [ ] Executar `python scripts/switch_config.py coexistence`
- [ ] Verificar `nvidia-smi` para confirmar uso de mem√≥ria
- [ ] Testar clonagem de voz
- [ ] Verificar se Ollama ainda funciona

### Para Speed:
- [ ] Parar Ollama se necess√°rio
- [ ] Executar `python scripts/switch_config.py speed`
- [ ] Verificar performance
- [ ] Testar clonagem de voz

## üéØ Recomenda√ß√µes

### Para Desenvolvimento:
- Use **Coexistence Mode** para ter ambos os servi√ßos dispon√≠veis
- A qualidade de √°udio √© suficiente para testes

### Para Produ√ß√£o:
- Use **Speed Mode** para m√°xima qualidade e velocidade
- Considere parar Ollama temporariamente se necess√°rio

### Para Demonstra√ß√µes:
- Use **Coexistence Mode** para mostrar ambas as funcionalidades
- A diferen√ßa de qualidade √© m√≠nima para a maioria dos casos

## üìä Backup e Restore

### Backup Autom√°tico
O script cria automaticamente backups antes de alternar:
```
config/settings_backup_1752708810.yaml
```

### Restore Manual
```bash
cp config/settings_backup_1752708810.yaml config/settings.yaml
```

## üîß Configura√ß√µes Avan√ßadas

### Ajustar Mem√≥ria GPU
Editar `config/settings_coexistence.yaml`:
```yaml
performance:
  gpu_memory_fraction: 0.25  # 25% em vez de 30%
```

### Ajustar Qualidade de √Åudio
```yaml
audio:
  quality: low      # low, medium, high
  sample_rate: 16000  # 16000, 22050, 44100
```

## ‚úÖ Conclus√£o

Com estas configura√ß√µes, consegue:
- ‚úÖ Correr Coqui TTS e Ollama Phi-3 simultaneamente
- ‚úÖ Manter performance aceit√°vel (~18s vs ~17s)
- ‚úÖ Alternar facilmente entre modos
- ‚úÖ Monitorizar uso de recursos
- ‚úÖ Manter qualidade suficiente para a maioria dos casos

A diferen√ßa de performance √© m√≠nima (1-2 segundos), mas a flexibilidade de ter ambos os servi√ßos dispon√≠veis √© valiosa para desenvolvimento e demonstra√ß√µes. 